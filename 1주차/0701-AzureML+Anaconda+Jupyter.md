# 1ì£¼ì°¨ 2022 07 01
</br>

## ğŸ“Œ Juypter Notebook

Jupyter Notebook: ì›¹ ë¸Œë¼ìš°ì €ì—ì„œ íŒŒì´ì¬ ì½”ë“œë¥¼ ì‘ì„±í•˜ê³  ì‹¤í–‰ê¹Œì§€ í•´ë³¼ ìˆ˜ ìˆëŠ” ê°œë°œë„êµ¬
</br>

### Anaconda ì„¤ì¹˜
ì•„ë‚˜ì½˜ë‹¤ë¥¼ ì„¤ì¹˜í•˜ë©´ íŒŒì´ì¬ê³¼ ë°ì´í„° ë¶„ì„ íŒ¨í‚¤ì§€ì˜ ì„¤ì¹˜ ë° ê´€ë¦¬ ê°€ëŠ¥

<img width="1310" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2022-07-03 á„‹á…©á„’á…® 11 13 39" src="https://user-images.githubusercontent.com/40768187/177043695-d66a8d25-7105-4948-b6c2-0c6f3d32012c.png">

</br>

### Jupyter notebook ì‹¤í–‰

- ê°•ì‚¬ë‹˜ì´ ì œê³µí•´ì¤€ Notebook íŒŒì¼ ë‹¤ìš´ë¡œë“œ

- Terminal ì°½ì—ì„œ ```cd Notebook```ìœ¼ë¡œ í•´ë‹¹ íŒŒì¼ì— ë“¤ì–´ê°„ í›„  ```jupyter notebook``` ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ jupyter notebook ì‹¤í–‰
<img width="937" alt="image" src="https://user-images.githubusercontent.com/40768187/177045792-a1e94e77-2cd9-412a-bb93-463f74563b03.png">
ìƒˆë¡œìš´ ì°½ì´ ì—´ë¦¬ë©´ ```New``` ë²„íŠ¼ì„ í´ë¦­ í›„ Python 3 ë¥¼ ì„ íƒí•˜ë©´ ì•„ë˜ì™€ ê°™ì€ í™”ë©´ì´ ë‚˜ì˜´
</br>ì´ì œ ì—¬ê¸°ì—ì„œ Python ì½”ë“œë¥¼ ì‘ì„±í•˜ì—¬ ì‹¤í–‰í•´ë³¼ ìˆ˜ ìˆìŒ

<img width="1734" alt="image" src="https://user-images.githubusercontent.com/40768187/177045858-7db32a8b-c5d4-4955-a0f2-ca68a6cc7241.png">

</br>

## ğŸ“Œ ë°ì´í„° ì „ì²˜ë¦¬

ì‚¬ì „ì— ë¬¸ì œ ì—†ì´ ì‹¤í—˜ì„ ì§„í–‰í•˜ê¸° ìœ„í•´ ë°ì´í„°ë¥¼ ì •ë¦¬í•˜ëŠ” ê²ƒ

1. Scaling
   - Min-Max Normalize
   - Standard Normalize (z-score)

2. Sampling
   - Random Up-Down Sampling
   - SMOTE

3. Dimensionality Reduction
   - PCA

4. Categorical Variable to Numeric Variable
   - Label Encording
   - One-hot Encoding

</br>

```python
import os       # í†µì§¸ë¡œ íŒŒì´ì¬ íŒ¨í‚¤ì§€ ê°€ì ¸ì˜¤ê¸°
from os.path import join        # íŒ¨í‚¤ì§€ ì¤‘ í•„ìš”í•œ ë¶€ë¶„(ê°ì²´)ë§Œ ê°€ì ¸ì˜¤ê¸°
import copy
import warnings
# os ë¼ëŠ” íŒ¨í‚¤ì§€ë¥¼ ë‹¤ ì˜¬ë¦¬ê¸°ì—” ë©”ëª¨ë¦¬ ë¶€í•˜ê°€ ì˜¬ ìˆ˜ ìˆìŒ. íŒŒì´ì¬ íŒ¨í‚¤ì§€ëŠ” ìš©ëŸ‰ì´ ë³´í†µ í¼

warnings.filterwarnings('ignore')   # ê²½ê³  ë©”ì„¸ì§€ ë¬´ì‹œ

import numpy as np
import pandas as pd     # ë°ì´í„° ì „ì²˜ë¦¬ í•  ë•Œ ì‚¬ìš©, = Excel ì´ë¼ê³  ë³´ë©´ ë¨

import sklearn

import matplotlib.pyplot as plt

abalone_path = join('data', 'abalone.txt')      # = data/abalone.txt
# í•˜ë“œì½”ë”©í•˜ì§€ ì•ŠëŠ” ì´ìœ  - ìœˆë„ìš°ì™€ ë¦¬ëˆ…ìŠ¤ëŠ” ê²½ë¡œ í‘œí˜„ë°©ì‹ì´ ë‹¤ë¦„

column_path = join('data', 'abalone_attributes.txt')

abalone_columns = list()
for l in open(column_path):
    abalone_columns.append(l.strip())
```

ì „ë³µ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ë¶ˆëŸ¬ì˜´
   - ì „ë³µ ë°ì´í„° ì…‹ì€ ìˆ˜ì»·, ì•”ì»·, ìœ ì•„ê¸° 3ê°œì˜ ë²”ì£¼ë¡œ ì´ë£¨ì–´ì§„ ë²”ì£¼í˜• ë³€ìˆ˜ì™€ ê¸¸ì´, ì§ê²½, ë†’ì´ ë¬´ê²Œ ë“± ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŒ

</br>

ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¨ í›„ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•  ë³€ìˆ˜ë“¤ê³¼ ë ˆì´ë¸”ë¡œ ì‚¬ìš©í•  ì„±ë³„ ë³€ìˆ˜ë¡œ ë‚˜ëˆ”

```python
data = pd.read_csv(abalone_path, header=None, names=abalone_columns)
label = data['Sex']
```
```data.head()``` ë¡œ 5ê°œì˜ ë°ì´í„°ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŒ

<img width="735" alt="image" src="https://user-images.githubusercontent.com/40768187/177045930-9c9f64ef-f10e-45c9-8eda-972a5208a5f9.png">

</br>

```data.shape``` ë¡œ ë°ì´í„°ì˜ ëª¨ì–‘ì„ í™•ì¸í•  ìˆ˜ ìˆìŒ (ë°ì´í„° ê°œìˆ˜, ì»¬ëŸ¼ ê°œìˆ˜)

<img width="295" alt="image" src="https://user-images.githubusercontent.com/40768187/177046031-58cf7a13-0fe7-41f5-b66a-771eee5df788.png">

</br>

```del data['Sex']``` ë¡œ Pandas DataFrame ì—ì„œ íŠ¹ì • ì»¬ëŸ¼ì„ ì œê±°</br>
```data.head()``` ë¡œ ì œê±°ë˜ì—ˆëŠ”ì§€ í™•ì¸

<img width="700" alt="image" src="https://user-images.githubusercontent.com/40768187/177046147-7d6bb31f-bedb-4554-8ff2-11bb8265ae76.png">

</br>

```data.describe()``` ë¡œ ê° ë³€ìˆ˜ë³„ í‰ê· , í‘œì¤€í¸ì°¨, ìµœëŒ€, ìµœì†Œ, ì‚¬ë¶„ìœ„ìˆ˜ ë“±ì˜ ê¸°ì´ˆ í†µê³„ëŸ‰ í™•ì¸ ê°€ëŠ¥

<img width="841" alt="image" src="https://user-images.githubusercontent.com/40768187/177046411-c01c0aa4-1cf9-453e-b549-5355183293ef.png">

</br>

```data.info()``` ë¡œ ê° ë³€ìˆ˜ë“¤ì˜ ìë£Œí˜• í™•ì¸ ê°€ëŠ¥

<img width="478" alt="image" src="https://user-images.githubusercontent.com/40768187/177046440-29511d09-2ffb-45b3-8b9c-047ae5977846.png">

</br>

### 1. Scailing

**ìŠ¤ì¼€ì¼ë§ì„ ì™œ í• ê¹Œ?**

ë³€ìˆ˜ì˜ í¬ê¸°ê°€ ë„ˆë¬´ ì‘ê±°ë‚˜, ë„ˆë¬´ í° ê²½ìš° í•´ë‹¹ ë³€ìˆ˜ê°€ Targetì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì´ ì œëŒ€ë¡œ í‘œí˜„ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ</br>

- Sklearn ì˜ ëŒ€í‘œì ì¸ ìŠ¤ì¼€ì¼ë§ í•¨ìˆ˜
  - Min-Max ìŠ¤ì¼€ì¼ë§: íŠ¹ì • ë³€ìˆ˜ì˜ ìµœëŒ€, ìµœì†Œ ê°’ìœ¼ë¡œ ì¡°ì ˆ
  - Standard ìŠ¤ì¼€ì¼ë§: z-ì •ê·œí™”ë¥¼ ì´ìš©

#### Min-Max ìŠ¤ì¼€ì¼ë§
- ê°’ì˜ ë²”ìœ„ê°€ 0 ~ 1 ì‚¬ì´ë¡œ ë³€ê²½ë¨
- X ì— ì¡´ì¬í•˜ëŠ” ì–´ë–¤ ê°€ì¥ ì‘ì€ ê°’ m ì— ëŒ€í•´ì„œ m ì€ Min(X) ì˜ ê°’ê³¼ ê°™ìŒ
- ë”°ë¼ì„œ ìŠ¤ì¼€ì¼ë§ í›„ m ì€ 0 ì´ ë˜ê³ , X ì— ì¡´ì¬í•˜ëŠ” ì–´ë–¤ ê°€ì¥ í° ê°’ M ì€ ë¶„ëª¨ì˜ ì‹ê³¼ ê°™ì•„ì§€ë¯€ë¡œ 1 ì´ ë¨
- ```data = (data - np.min(data)) / (np.max(data) - np.min(data))```

</br>

1. ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ë° ì •ì˜

```python
from sklearn.preprocessing import MinMaxScaler
# SKlearn ì—ì„œ Min-Max Scaler ëŠ” preprocessing íŒ¨í‚¤ì§€ì— ìœ„ì¹˜í•¨

mMscaler = MinMaxScaler()
```

2. ë°ì´í„°ì—ì„œ íŠ¹ì§• ì°¾ê¸° (Min, Max ê°’)

```python
mMscaler.fit(data)
```

3. ë°ì´í„° ë³€í™˜

```python
mMscaled_data = mMscaler.fit_transform(data)    # ë°ì´í„° ë¼ì›Œë§ì¶°ì„œ ë³€í™˜

mMscaled_data.min()     # 0.0

mMscaled_data.max()     # 1.0
```

4. ê²°ê³¼

```python
data.head()
```

```python
mMscaled_data = pd.DataFrame(mMscaled_data, columns = data.columns)
mMscaled_data.head()
```

<img width="736" alt="image" src="https://user-images.githubusercontent.com/40768187/177047080-0d1932ca-c3f7-4616-ba90-cad8803055ba.png">

</br>

### 2. Sampling

**ìƒ˜í”Œë§ì„ ì™œ í• ê¹Œ?**
í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œ ë•Œë¬¸.
</br>
í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œ?
ë¶„ë¥˜ë¥¼ ëª©ì ìœ¼ë¡œ í•˜ëŠ” ë°ì´í„° ì…‹ì— í´ë˜ìŠ¤ ë¼ë²¨ì˜ ë¹„ìœ¨ì´ ê· í˜•ì„ ë§ì¶”ì§€ ì•Šê³ , í•œìª½ìœ¼ë¡œ ì¹˜ìš°ì¹œ ê²½ìš°

- ìƒ˜í”Œë§ ì¢…ë¥˜
  - Oversampling: ì ì€ í´ë˜ìŠ¤ì˜ ë°ì´í„° ìˆ˜ë¥¼ ì¦ê°€ì‹œí‚´ (ë” ë§ì´ ì‚¬ìš©)
  - Undersampling: ë§ì€ í´ë˜ìŠ¤ì˜ ë°ì´í„° ìˆ˜ë¥¼ ê°ì†Œì‹œí‚´

#### Random Over, Under Sampling

ê°€ì¥ ì‰½ê²Œ ìƒ˜í”Œë§ í•˜ëŠ” ë°©ë²•ì€ ëœë¤ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì„ íƒí•˜ì—¬, ë³µì œí•˜ê±°ë‚˜ ì œê±°í•˜ëŠ” ë°©ì‹.
</br>
ìœ„ ë°©ì‹ì€ ë‹¤ìŒê³¼ ê°™ì€ ë¬¸ì œì ì´ ì¡´ì¬
- ë³µì œí•˜ëŠ” ê²½ìš°, ì„ íƒëœ ë°ì´í„°ì˜ ìœ„ì¹˜ì— ë˜‘ê°™ì´ ì ì„ ì°ê¸° ë•Œë¬¸ì— ë°ì´í„° ê³¼ì í•©ì´ ë  ìˆ˜ ìˆìŒ
- ì œê±°í•˜ëŠ” ê²½ìš°, ë°ì´í„°ì…‹ì´ ê°€ì§€ê³  ìˆëŠ” ì •ë³´ì˜ ì†ì‹¤ì´ ìƒê¸¸ ìˆ˜ ìˆìŒ

1. ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ë° ì •ì˜

```python
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler

ros = RandomOverSampler()
rus = RandomUnderSampler()
```

ì˜¤ë¥˜ ë°œìƒ ì‹œ ì•„ë˜ì™€ ê°™ì€ ëª…ë ¹ì–´ ì‘ì„±
```python
!pip install imblearn
# shell ì‹¤í–‰í•˜ëŠ” ëª…ë ¹ì–´?
# ! ì´í›„ì— ë‚˜ì˜¤ëŠ” ë‚´ìš©ë“¤ì„ ì»¤ë§¨ë“œì°½ì—ì„œ ì…ë ¥í•˜ëŠ”ê²ƒê³¼ ê°™ì´ ì²˜ë¦¬ í•´ë‹¬ë¼ëŠ” ëœ»
```

2. ë°ì´í„°ì—ì„œ íŠ¹ì§• ì°¾ê¸° (ë°ì´í„° ë¹„ìœ¨), ë°ì´í„° ìƒ˜í”Œë§

```python
# ë°ì´í„°ì—ì„œ íŠ¹ì§•ì„ í•™ìŠµí•¨ê³¼ ë™ì‹œì— ë°ì´í„° ìƒ˜í”Œë§
# Over ìƒ˜í”Œë§
oversampled_data, oversampled_label = ros.fit_resample(data, label)
oversampled_data = pd.DataFrame(oversampled_data, columns=data.columns)
    # DataFrame -> í‘œ í˜•íƒœë¡œ ë˜ì–´ìˆëŠ” ë°ì´í„°ë¡œ ë§Œë“¬ (pandas)

# Under ìƒ˜í”Œë§
undersampled_data, undersampled_label = rus.fit_resample(data, label)
undersampled_data = pd.DataFrame(undersampled_data, columns=data.columns)
```

3. ê²°ê³¼

```python
print('ì›ë³¸ ë°ì´í„°ì˜ í´ë˜ìŠ¤ ë¹„ìœ¨ \n{}'.format(pd.get_dummies(label).sum()))
print('\nRandom Over ìƒ˜í”Œë§ ê²°ê³¼ \n{}'.format(pd.get_dummies(oversampled_label).sum()))
print('\nRandom Under ìƒ˜í”Œë§ ê²°ê³¼ \n{}'.format(pd.get_dummies(undersampled_label).sum()))
```

<img width="1087" alt="image" src="https://user-images.githubusercontent.com/40768187/177157512-af0c7147-cb1f-4162-beb5-0805609b5f52.png">

ë¹„ìœ¨ì´ ë¹„ìŠ·í•˜ê²Œ ë§ì¶°ì§„ ê²ƒì„ ë³¼ ìˆ˜ ìˆìŒ

</br>

### 3. SMOTE (Synthetic Minority Oversampling Technique)

**SMOTE ë¥¼ ì™œ í• ê¹Œ?**

ìœ„ì—ì„œ ì§„í–‰í–ˆë˜ Over, Under ìƒ˜í”Œë§ì€ ë°ì´í„°ì˜ ì¤‘ë³µìœ¼ë¡œ ì¸í•œ ê³¼ì í•© ë¬¸ì œì™€ ë°ì´í„° ì†ì‹¤ì˜ ë¬¸ì œê°€ ìˆì—ˆìŒ</br>
ì´ëŸ° ë¬¸ì œë¥¼ ìµœëŒ€í•œ í”¼í•˜ë©´ì„œ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì´ SMOTE

**SMOTE ?**

ìˆ˜ê°€ ì ì€ í´ë˜ìŠ¤ì˜ ì ì„ í•˜ë‚˜ ì„ íƒí•´ k ê°œì˜ ê°€ê¹Œìš´ ë°ì´í„° ìƒ˜í”Œì„ ì°¾ê³ ,  ê·¸ ì‚¬ì´ì— ìƒˆë¡œìš´ ì ì„ ìƒì„±í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜


![KakaoTalk_Photo_2022-07-04-22-07-48](https://user-images.githubusercontent.com/40768187/177161220-368f1938-e2f4-4917-9041-84b0c2165923.png)

</br>

0. ì‹œê°í™”ë¥¼ í†µí•´ ìƒì„±í•œ ë°ì´í„° í™•ì¸

```python
from sklearn.datasets import make_classification
data, label = make_classification(n_samples=1000, n_features=2, n_informative=2,    # feature 2ê°œ => x, y ì¶•
                           n_redundant=0, n_repeated=0, n_classes=3,   # 3ê°€ì§€ë¡œ ë¶„ë¥˜
                           n_clusters_per_class=1,
                           weights=[0.05, 0.15, 0.8],   # 0.05 : 0.15 : 0.8 ì˜ ê°€ì¤‘ì¹˜ ê°’ìœ¼ë¡œ ë°ì´í„° ìƒì„±
                           class_sep=0.8, random_state=2019)
# make_classification : ì‹¤í—˜í•  ë•Œ í•„ìš”í•œ ë°ì´í„°ë¥¼ ê°•ì œë¡œ ë§Œë“¤ì–´ ì£¼ëŠ” ê²ƒ. ì…ë§ì— ë§ëŠ” ë¶„ë¥˜í˜• ìƒ˜í”Œ ë°ì´í„° ë§Œë“¤ì–´ì¤Œ
```

```python
fig = plt.Figure(figsize=(12,6))
plt.scatter(data[:, 0], data[:, 1], c=label, linewidth=1, edgecolor='black')
plt.show()
```

<img width="795" alt="image" src="https://user-images.githubusercontent.com/40768187/177162297-f4ebf533-c74c-4bdb-8e77-9d5bd64b081a.png">

+) SMOTE ëŠ” imblearn ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ over_sampling íŒ¨í‚¤ì§€ì— ì¡´ì¬

1. ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ë° ì •ì˜

```python
# ë¶„í¬ì—ì„œ ë–¨ì–´ì ¸ ìˆëŠ” ì¹œêµ¬ì™€ ê·¸ ê·¼ì²˜ì— ì„ ì„ê¸‹ê³  ì„  ìœ„ì— ë°ì´í„° ì±„ìš°ê¸°

from imblearn.over_sampling import SMOTE
## k_neighbors íŒŒë¼ë¯¸í„°ë¡œ ê°€ê¹Œìš´ ë°ì´í„° ìƒ˜í”Œì˜ ìˆ˜ë¥¼ ê²°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
smote = SMOTE(k_neighbors=5)    # ì´ì›ƒ 5ê°œë¥¼ ë³´ê³  ë°ì´í„° ì±„ìš°ê¸°
```

2. ë°ì´í„° íŠ¹ì§• ì°¾ê¸° (ë°ì´í„° ë¹„ìœ¨), ë°ì´í„° ìƒ˜í”Œë§

```python
smoted_data, smoted_label = smote.fit_resample(data, label)
```

3. ê²°ê³¼

```python
print('ì›ë³¸ ë°ì´í„°ì˜ í´ë˜ìŠ¤ ë¹„ìœ¨ \n{}'.format(pd.get_dummies(label).sum()))
print('\nSMOTE ê²°ê³¼ \n{}'.format(pd.get_dummies(smoted_label).sum()))
```

```python
fig = plt.Figure(figsize=(12,6))
plt.scatter(smoted_data[:, 0], smoted_data[:, 1], c=smoted_label, linewidth=1, edgecolor='black')
plt.show()
```

<img width="923" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2022-07-04 á„‹á…©á„’á…® 10 33 20" src="https://user-images.githubusercontent.com/40768187/177165734-49363d24-84c8-4599-8383-37a02f0a2434.png">

ì´ì „ ê²ƒ(0ë²ˆ ì±•í„°) ê³¼ ë¹„êµí•´ ë³´ë©´ 3ê°œë¡œ ë¶„ë¥˜ ë˜ì–´ ìˆëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŒ
</br> ì´ì „ì˜ 2ê°€ì§€ ìƒ˜í”Œë§ ë°©ë²•ë³´ë‹¤ ë°ì´í„°ì˜ ë¶„í¬ë¥¼ ìœ ì§€í•˜ë©´ì„œ ìƒˆë¡œìš´ ìœ„ì¹˜ì— ë°ì´í„° ìƒì„± ê°€ëŠ¥

</br>

### 4. Dimensionality Reduction

**ì°¨ì› ì¶•ì†Œë¥¼ ì™œ í• ê¹Œ?**

- **ì°¨ì›ì˜ ì €ì£¼?**

    ì €ì°¨ì›ì—ì„œëŠ” ì¼ì–´ë‚˜ì§€ ì•ŠëŠ” í˜„ìƒë“¤ì´ ê³ ì°¨ì›ì—ì„œ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê±°ë‚˜ ë‹¤ë£° ë•Œ ìƒê²¨ë‚˜ëŠ” í˜„ìƒ

ê³ ì°¨ì›ìœ¼ë¡œ ì¦ê°€í•  ìˆ˜ë¡ ê³µê°„ì˜ í¬ê¸° ì¦ê°€, ì´ì— ë”°ë¼ ë°ì´í„°ëŠ” í•´ë‹¹ ê³µê°„ì— í•œì •ì ìœ¼ë¡œ ìœ„ì¹˜ë˜ë©´ì„œ ë¹ˆ ê³µê°„ì´ ë§ì•„ì§

ì´ëŸ¬í•œ ì´ìœ ë¡œ ë°ì´í„°ì˜ ì°¨ì›ì´ ë„ˆë¬´ í° ê²½ìš°, í•„ìš” ì—†ëŠ” ë³€ìˆ˜ë¥¼ ì œê±°í•˜ê³ , ê³¼ì í•©ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ë°ì´í„°ì˜ ì°¨ì›ì„ ì¶•ì†Œí•¨</br>
ë˜ëŠ”, ì‚¬ëŒì´ ì¸ì‹í•  ìˆ˜ ìˆëŠ” ì°¨ì›ì€ 3ì°¨ì›ì´ ìµœëŒ€ì´ë¯€ë¡œ ë°ì´í„°ì˜ ì‹œê°í™”ë¥¼ ìœ„í•´ ì¶•ì†Œí•˜ê¸°ë„ í•¨

- ì°¨ì› ì¶•ì†Œ ê¸°ë²• ì¢…ë¥˜
  - ì£¼ ì„±ë¶„ ë¶„ì„ (Principal Component Analysis, PCA)
    
    ì—¬ëŸ¬ ì°¨ì›ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ë°ì´í„°ë¥¼ ê°€ì¥ ì˜ í‘œí˜„í•˜ëŠ” ì¶•ìœ¼ë¡œ Projection í•´ì„œ ì°¨ì›ì„ ì¶•ì†Œ</br>
    ë°ì´í„°ë¥¼ ì˜ í‘œí˜„í•˜ëŠ” ì¶• = ë°ì´í„°ì˜ ë¶„ì‚°ì„ ì˜ í‘œí˜„í•˜ëŠ” ì¶•</br>

    ex) ì êµ°ì„ ê¸°ê´€ì´ìœ¼ë¡œ ì  ë•Œ ì •ë©´ë³´ë‹¤ëŠ” ì–‘ìª½ ëì—ì„œ ì˜ëŠ” ê²ƒì´ ì˜ ë§ìŒ</br>

    <img width="218" alt="image" src="https://user-images.githubusercontent.com/40768187/177169409-5fabc60b-c593-4912-9e55-9406a3f4b637.png">

    ê¸°ë³¸ì ìœ¼ë¡œ ì£¼ì„±ë¶„(PC) ì€ ë°ì´í„° ì…‹ì„ íŠ¹ì´ê°’ ë¶„í•´ë¥¼ í†µí•´ ì¶”ì¶œëœ ê³ ìœ  ë²¡í„°</br>
    ê° ê³ ìœ  ë²¡í„°ë“¤ì€ ì„œë¡œ ì§êµì„±ì„ ë„ê¸° ë•Œë¬¸ì— ë°ì´í„°ë¥¼ ì£¼ì„±ë¶„ìœ¼ë¡œ Projection ì‹œì¼°ì„ ë•Œ ì„œë¡œ ë…ë¦½ì ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì˜ í‘œí˜„í•  ìˆ˜ ìˆìŒ

    PCA ë‹¨ì ìœ¼ë¡œëŠ” ë–¨ì–´ëœ¨ë¦° ì£¼ì„±ë¶„ì´ ì–´ë–¤ ì»¬ëŸ¼ì¸ì§€ ì„¤ëª…í•  ìˆ˜ ì—†ìŒ

  - ì£¼ ì„±ë¶„ ë¶„ì„ì˜ ë‹¨ê³„
    1. ê° ì»¬ëŸ¼ë“¤ì˜ ê°’ì˜ ë²”ìœ„ë¥¼ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë¥¼ ì‚¬ìš©í•´ ì •ê·œí™” (ìŠ¤ì¼€ì¼ë§)
    2. ë°ì´í„°ì˜ ê³µë¶„ì‚° ê³„ì‚°
    3. ê³µë¶„ì‚° í–‰ë ¬ì— ëŒ€í•´ íŠ¹ì´ê°’ ë¶„í•´ë¥¼ í•˜ì—¬ ì£¼ì„±ë¶„ (ê³ ìœ  ë²¡í„°) ì™€ ê³ ìœ  ê°’ì„ ì–»ì–´ëƒ„
    4. ì£¼ì„±ë¶„ê³¼ ëŒ€ì‘ë˜ëŠ” ê³ ìœ ê°’ì€ ì£¼ì„±ë¶„ì´ ë°ì´í„°ì˜ ë¶„ì‚°ì„ í‘œí˜„í•˜ëŠ” ì •ë„ì˜ ì²™ë„ë¡œ ì‚¬ìš©ë¨</br>
        ë”°ë¼ì„œ ê³ ìœ ê°’ì˜ í¬ê¸°ì™€ ë¹„ìœ¨ì„ ë³´ê³  ëª‡ê°œì˜ ì£¼ì„±ë¶„ì„ ì„ íƒí•  ê²ƒì¸ì§€ ë˜ëŠ” ì›í•˜ëŠ” ì°¨ì›ì˜ ê°œìˆ˜ë§Œí¼ ì£¼ì„±ë¶„ ì„ íƒ
    5. ì„ íƒí•œ ì£¼ì„±ë¶„ìœ¼ë¡œ ëª¨ë“  ë°ì´í„°ë¥¼ Projection ì‹œì¼œ ë°ì´í„° ì°¨ì› ì¶•ì†Œ

</br>

**Projection? (ì‚¬ì˜)**

ë²¡í„° b ë¥¼ ë²¡í„° a ì— ì‚¬ì˜í•œë‹¤ëŠ” ê²ƒì€ ë²¡í„° a ì— ëŒ€í•´ ìˆ˜ì§ì¸ ë°©í–¥ìœ¼ë¡œ ë²¡í„° b ë¥¼ ë–¨ì–´ëœ¨ë¦¬ëŠ” ê²ƒì„ ì˜ë¯¸</br>
ê°„ë‹¨íˆ ë§í•´, ë²¡í„° b ì˜ ê·¸ë¦¼ìë¥¼ ë²¡í„° a ì— ë–¨ì–´ëœ¨ë¦° ê²ƒ

<img width="209" alt="image" src="https://user-images.githubusercontent.com/40768187/177171448-cb773253-e473-4288-9463-59f74641cc15.png"> <img width="473" alt="image" src="https://user-images.githubusercontent.com/40768187/177172674-8d3ef0e7-a441-4a08-9efa-bf9d9d239b72.png">

v ì˜ u ë¡œ ìœ„ë¡œì˜ ì‚¬ì˜

**PCAì˜ ê¸°ë³¸ ì›ë¦¬ëŠ” ë°ì´í„°ì˜ ë¶„ì‚°ì„ ê°€ì¥ ì˜ í‘œí˜„í•˜ëŠ” ë²¡í„°ë¥¼ ì°¾ì•„ í•´ë‹¹ ë²¡í„°ì— ë°ì´í„°ë“¤ì„ ì‚¬ì˜ ì‹œí‚¤ëŠ” ê²ƒ**

0. ë°ì´í„° ì‚´í´ë³´ê¸°

```python
from sklearn.datasets import load_digits
digits = load_digits()      # MNIST? ì†ê¸€ì”¨ ë°ì´í„° ==> ë°”ì´ë¸”ì ì¸ ë°ì´í„° (load_digits())
```

```python
print(digits.DESCR)
```

<img width="795" alt="image" src="https://user-images.githubusercontent.com/40768187/177174639-0ab58d5c-7521-4490-96fc-ce9759c65dec.png">

</br>

```python
data = digits.data
label = digits.target   # ë‹µ (ì˜ˆì¸¡í•œ ë‹µ)
```

```python
data.shape  # (ì „ì²´ ë°ì´í„°ìˆ˜, ì»¬ëŸ¼ ìˆ˜)
```

ìˆ«ì ì´ë¯¸ì§€ê°€ 64 ì°¨ì› ë²¡í„°ë¡œ í‘œí˜„ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì´ë¯¸ì§€ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ì„œëŠ” (8,8) í–‰ë ¬ë¡œ ë³€í™˜í•´ì¤˜ì•¼ í•¨

```python
plt.imshow(data[0].reshape((8,8)))      # 8 x 8 ë¡œ ì˜ë¼ì£¼ê¸°
print('Label : {}'.format(label[0]))
```

<img width="411" alt="image" src="https://user-images.githubusercontent.com/40768187/177174728-e77c6417-f9aa-4141-a55c-95f4b80de553.png"> <img width="341" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2022-07-04 á„‹á…©á„’á…® 11 26 25" src="https://user-images.githubusercontent.com/40768187/177174511-55542a2e-ebf8-4a02-9c94-57bc0b188737.png">

0ë²ˆì§¸ ë°ì´í„°ëŠ” ì´ë¯¸ì§€ ìƒìœ¼ë¡œ 0ìœ¼ë¡œ ë³´ì´ê³ , ë¼ë²¨ë„ 0ì¸ ê²ƒì„ í™•ì¸í•¨</br>
PCAë¥¼ í†µí•´ 64 ì°¨ì› ë°ì´í„°ë¥¼ 2ì°¨ì› ë°ì´í„°ë¡œ ì¶•ì†Œ ì‹œí‚¬ ì˜ˆì •
- ì—¬ê¸°ì—ì„œ digits ë°ì´í„°ì˜ ê° í”½ì…€(ë³€ìˆ˜)ì˜ ìŠ¤ì¼€ì¼ì€ 0 ~ 16 ìœ¼ë¡œ ê°™ìœ¼ë¯€ë¡œ ì¶”ê°€ì ì¸ ì •ê·œí™” ì§„í–‰X

</br>

1. ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ë° ì •ì˜

```python
from sklearn.decomposition import PCA
pca = PCA(n_components=2)   # 2ì°¨ì›
```

2. ë°ì´í„°ì—ì„œ íŠ¹ì§• ì°¾ê¸° (ì£¼ ì„±ë¶„ ì°¾ê¸°)

```python
pca.fit(data)
```

3. ë°ì´í„° ë³€í™˜ (ì£¼ ì„±ë¶„ìœ¼ë¡œ ë°ì´í„° ì‚¬ì˜)

```python
new_data = pca.transform(data)
```

4. ê²°ê³¼

```python
print('ì›ë³¸ ë°ì´í„°ì˜ ì°¨ì› \n{}'.format(data.shape))
print('\nPCAë¥¼ ê±°ì¹œ ë°ì´í„°ì˜ ì°¨ì› \n{}'.format(new_data.shape))
```

```python
plt.scatter(new_data[:,0], new_data[:, 1], c=label, linewidth=1, edgecolor='black')
plt.show()
```

<img width="809" alt="image" src="https://user-images.githubusercontent.com/40768187/177181745-9e848e1a-fc6c-469f-afb2-1e64701ff3e8.png">

</br>

### 5. Catagorical Variable to Numeric Variable

ë²”ì£¼í˜• ë³€ìˆ˜ë¥¼ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë¡œ ë‚˜íƒ€ë‚´ëŠ” ë°©ë²•

**ë²”ì£¼í˜• ë³€ìˆ˜?**
- ì°¨ì˜ ë“±ê¸‰ì„ ë‚˜íƒ€ë‚´ëŠ” [ì†Œí˜•, ì¤‘í˜•, ëŒ€í˜•] ì²˜ëŸ¼ í‘œí˜„ë˜ëŠ” ë³€ìˆ˜
- ì£¼ë¡œ ë°ì´í„° ìƒì—ì„œ ë¬¸ìì—´ë¡œ í‘œí˜„ë˜ëŠ” ê²½ìš°ê°€ ë§ê³ , ë¬¸ìì™€ ìˆ«ìê°€ ë§¤í•‘ë˜ëŠ” í˜•íƒœë¡œ í‘œí˜„ë˜ê¸°ë„ í•¨

**ë²”ì£¼í˜• ë³€ìˆ˜ë¥¼ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë¡œ ë‚˜íƒ€ë‚´ëŠ” ì´ìœ ê°€ ë­˜ê¹Œ?**

ê·¸ëƒ¥ ë¶„ë¥˜ì¼ ë¿ì¸ë° ì»´í“¨í„°ëŠ” í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„ì„ í•˜ë©´ì„œ ì‹œê°„ì´ ì˜¤ë˜ê±¸ë¦¬ê²Œ ë¨

**ì¢…ë¥˜**
- Label Encoding: n ê°œì˜ ë²”ì£¼í˜• ë°ì´í„°ë¥¼ 0~n-1 ì˜ ì—°ì†ì ì¸ ìˆ˜ì¹˜ ë°ì´í„°ë¡œ í‘œí˜„
  - ex) ì†Œí˜• - 0, ì¤‘í˜• - 1, ëŒ€í˜• - 2
  - Sklearn ì˜ preprocessing íŒ¨í‚¤ì§€ì— ì¡´ì¬
- One-hot Encoding:  n ê°œì˜ ë²”ì£¼í˜• ë°ì´í„°ë¥¼ n ê°œì˜ ë¹„íŠ¸ (0,1) ë²¡í„°ë¡œ í‘œí˜„
  - ex) ì†Œí˜• - [1,0,0], ì¤‘í˜• - [0,1,0], ëŒ€í˜• - [0,0,1]
  - ì„œë¡œ ë‹¤ë¥¸ ë²”ì£¼ì— ëŒ€í•´ì„œëŠ” ë²¡í„° ë‚´ì ì„ ì·¨í–ˆì„ ë•Œ ë‚´ì  ê°’ 0
  - ì´ëŠ” ì„œë¡œ ë‹¤ë¥¸ ë²”ì£¼ ë°ì´í„°ëŠ” ë…ë¦½ì ì¸ ê´€ê³„ë¼ëŠ” ê²ƒì„ ëœ»í•¨
  - ë¬¸ì¥ì˜ ì°¨ì›ì„ í¼ì³ë†“ê³  encoding í•˜ëŠ” ê²ƒ
  - ex) ì˜í™” ë¦¬ë·°ë¥¼ ê¸ì–´ì„œ encoding í•´ì„œ ë¶„ì„
  - Sklearn ì˜ preprocessing íŒ¨í‚¤ì§€ì— ì¡´ì¬

<img width="550" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2022-07-05 á„‹á…©á„Œá…¥á†« 12 28 01" src="https://user-images.githubusercontent.com/40768187/177184214-fe563f51-dafa-4657-9275-197415fb7e4d.png">

#### 1. Label Encoding

ì „ë³µ ë°ì´í„°ì˜ targetì´ì—ˆë˜, ì„±ë³„ ë³€ìˆ˜ë¥¼ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë¡œ ë³€í™˜í•˜ê¸°

0. ë°ì´í„° ì‚´í´ë³´ê¸°

```python
data = pd.read_csv(abalone_path, header=None, names=abalone_columns)
label = data['Sex']
del data
```

```python
label.head()
```

<img width="707" alt="image" src="https://user-images.githubusercontent.com/40768187/177184675-f1d15ad8-a7ee-4e8c-9007-fdeeb0e395cf.png">

1. ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ë° ì •ì˜í•˜ê¸°

```python
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
```

2. ë°ì´í„°ì—ì„œ íŠ¹ì§• ì°¾ê¸° (ë²”ì£¼ì˜ ìˆ˜)

```python
le.fit(label)
```

3. ë°ì´í„° ë³€í™˜ (ë²”ì£¼í˜• ë³€ìˆ˜ë¥¼ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë¡œ)

```python
label_encoded_label = le.transform(label)
```

4. ê²°ê³¼

```python
result = pd.DataFrame(data = np.concatenate([label.values.reshape((-1,1)), 
label_encoded_label.reshape((-1, 1))], axis=1), columns=['label', 'label_encoded'])

result.head(10)
```

```python
le.inverse_transform(label_encoded_label)
```

<img width="1078" alt="image" src="https://user-images.githubusercontent.com/40768187/177185185-60028e3e-2db6-404b-bfac-dbf078143e33.png">

</br>

#### 2. Label Encoding

1. ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ë° ì •ì˜

```python
from sklearn.preprocessing import OneHotEncoder
ohe = OneHotEncoder(sparse=False)
```

2. ë°ì´í„°ì—ì„œ íŠ¹ì§• ì°¾ê¸° (ë²”ì£¼ì˜ ìˆ˜)

```python
ohe.fit(label.values.reshape((-1, 1)))
```

3. ë°ì´í„° ë³€í™˜ (ë²”ì£¼í˜• ë³€ìˆ˜ë¥¼ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë¡œ)

```python
one_hot_encoded = ohe.transform(label.values.reshape((-1,1)))
```

4. ê²°ê³¼

```python
columns = np.concatenate([np.array(['label']) , ohe.categories_[0]])

result = pd.DataFrame(data = np.concatenate([label.values.reshape((-1,1)), 
one_hot_encoded.reshape((-1, 3))], axis=1), columns=columns)

result.head(10)
```

<img width="1071" alt="image" src="https://user-images.githubusercontent.com/40768187/177185877-1f7ede62-1082-456a-bff6-895a14faae38.png">

</br>


#### ì¶”ê°€ ë©”ëª¨
- ì»¬ëŸ¼ ê°œìˆ˜ = ì»¬ëŸ¼ ê°œìˆ˜ ì°¨ì›
- ë°ì´í„° ëˆ„ë½ì€ ì—„ì²­ë‚œ ë¬¸ì œë¥¼ ì¼ìœ¼í‚´ => **ë…¸ì´ì¦ˆ** ë¼ê³  í•¨
- ë°ì´í„°ê°€ ì¶©ë¶„íˆ ë§ìœ¼ë©´ ëˆ„ë½ëœ ë°ì´í„°ê°€ ìˆëŠ” ì—´ì„ ì§€ì›Œë²„ë ¤ë„ ë¨
- ë°ì´í„°ê°€ ë¶€ì¡±í•  ë•Œ ì‚´ë¦´ë ¤ë©´ ì˜í–¥ì„ ì•ˆë¯¸ì¹˜ê³  ì±„ì›Œì•¼í•¨ -> í‰ê·  ë„£ê¸°
- ìœ„ ë°©ì‹ì€ ì œëŒ€ë¡œ ëœ ì‹¤í—˜ì´ ë  ìˆ˜ ì—†ìŒ. ê·¸ë˜ì„œ MinMaxScaler, PCA ë“±ì˜ ë°©ì‹ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒ

</br>

## ğŸ“Œ Clustering (êµ°ì§‘í™”)

ë°ì´í„°ë¥¼ ë§‰ ë‚˜ëˆ ë³´ë‹¤ê°€ ì˜ ë‚˜ëˆ ì¡Œì„ ë•Œì˜ ê²°ê³¼ì˜ ì„  ì°¾ê¸°

- ì¢…ë¥˜
  - k-means Clustering
    - ê° í´ëŸ¬ìŠ¤í„°ì— í• ë‹¹ëœ ë°ì´í„° í¬ì¸íŠ¸ë“¤ì˜ í‰ê·  ì¢Œí‘œë¥¼ ì´ìš©í•´ ì¤‘ì‹¬ì ì„ ë°˜ë³µì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ë©° í´ëŸ¬ìŠ¤í„° í˜•ì„±
    - ì ê³¼ ì  ì‚¬ì˜ ê±°ë¦¬ ì¸¡ì • ë°©ì‹
      - Manhattan Distance: ê° ì¶•ì— ëŒ€í•´ ìˆ˜ì§ìœ¼ë¡œë§Œ ì´ë™í•˜ì—¬ ê³„ì‚°í•˜ëŠ” ê±°ë¦¬ ì¸¡ì •ë°©ì‹
      - Euclidean Distance: ì ê³¼ ì  ì‚¬ì´ì˜ ê°€ì¥ ì§§ì€ ê±°ë¦¬ë¥¼ ê³„ì‚°í•˜ëŠ” ê±°ë¦¬ ì¸¡ì •ë°©ì‹
  
  - Hierarchical Clustering
    - ê±°ë¦¬(Distance) ë˜ëŠ” ìœ ì‚¬ë„(Similarity)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë¥¼ í˜•ì„±
    - k-means Clustering ê³¼ ë‹¤ë¥´ê²Œ í´ëŸ¬ìŠ¤í„°ì˜ ìˆ˜ë¥¼ ì„¤ì •í•´ ì¤„ í•„ìš” ì—†ìŒ
    - í´ëŸ¬ìŠ¤í„° í˜•íƒœë¥¼ ì‹œê°ì ìœ¼ë¡œ í‘œí˜„í•´ì£¼ëŠ” ë´ë“œë¡œê·¸ë¨ì„ í†µí•´ ì ì ˆí•œ í´ëŸ¬ìŠ¤í„° ìˆ˜ ì„ íƒ ê°€ëŠ¥
    - Bottom-Up ë°©ì‹ì˜ ```Agglomerative Method```ì™€ Top-Down ë°©ì‹ì˜ ```Divisive Method```ë¡œ ë‚˜ë‰¨
    - ê°€ì¥ ê°€ê¹Œìš´ í´ëŸ¬ìŠ¤í„°ë¥¼ ì°¾ëŠ” ë°©ì‹
      - Single Linkage: ë‘ í´ëŸ¬ìŠ¤í„° ë‚´ì˜ ê°€ì¥ ê°€ê¹Œìš´ ì  ì‚¬ì´ì˜ ê±°ë¦¬
      - Complete Linkage: ë‘ í´ëŸ¬ìŠ¤í„° ë‚´ì˜ ê°€ì¥ ë¨¼ ì  ì‚¬ì´ì˜ ê±°ë¦¬
      - Average Linkage: ë‘ í´ëŸ¬ìŠ¤í„° ë‚´ì˜ ëª¨ë“  ì  ì‚¬ì´ì˜ í‰ê·  ê±°ë¦¬

- í‰ê°€
  - Silhouette: í•œ í´ëŸ¬ìŠ¤í„° ì•ˆì˜ ë°ì´í„°ë“¤ì´ ë‹¤ë¥¸ í´ëŸ¬ìŠ¤í„°ì™€ ë¹„êµí•´ ì–¼ë§ˆë‚˜ ë¹„ìŠ·í•œê°€ë¥¼ ë‚˜íƒ€ëƒ„
    - í´ëŸ¬ìŠ¤í„° ê°„ ê±°ë¦¬ -> ë†’ì„ ìˆ˜ë¡ ì¢‹ì€ ê²ƒ

### 1. k-means Clustering

- ì½”ë“œ
  
  ```python
    from sklearn.preprocessing import MinMaxScaler
    scaler = MinMaxScaler()
    data = scaler.fit_transform(data)

    from sklearn.decomposition import PCA
    pca = PCA(n_components=2)
    data = pca.fit_transform(data)

    data.shape

    from sklearn.cluster import KMeans
    kmeans = KMeans(n_clusters=3)

    kmeans.fit(data)

    cluster = kmeans.predict(data)

    plt.scatter(data[:, 0], data[:, 1], c=cluster, linewidth=1, edgecolor='black')
    plt.show()
  ```

- ê²°ê³¼
  
<img width="407" alt="image" src="https://user-images.githubusercontent.com/40768187/177189580-e3667151-756c-4e14-9972-a883d256e341.png">


### 2. k-means Clustering

#### Single Linkage

- ì½”ë“œ

    ```python
    from sklearn.cluster import AgglomerativeClustering
    single_clustering = AgglomerativeClustering(n_clusters=3, linkage='single')

    single_clustering.fit(data)

    single_cluster = single_clustering.labels_
    ```

- ê²°ê³¼
  
  ì‚°ì ë„

  ```python
    plt.scatter(data[:,0], data[:,1], c=single_cluster)
    plt.title('Sklearn Single Linkage Hierarchical Clustering')
    plt.show()
  ```

    <img width="385" alt="image" src="https://user-images.githubusercontent.com/40768187/177190870-5b3fdf82-a97c-4044-87ba-c82c7a0e63b6.png">

  ë´ë“œë¡œê·¸ë¨

  ```python
  from scipy.cluster.hierarchy import dendrogram
    plt.figure(figsize=(10,10))

    # Hierarchical Clusteringì˜ ìì‹ ë…¸ë“œ
    children = single_clustering.children_

    # ê° ìì‹ ë…¸ë“œê°„ì˜ ê±°ë¦¬ ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆì§€ ì•Šê¸° ë•Œë¬¸ì—, ê· ì¼í•˜ê²Œ ê·¸ë¦¬ë„ë¡ í•©ë‹ˆë‹¤.
    distance = np.arange(children.shape[0])

    # ê° í´ëŸ¬ìŠ¤í„° ë‹¨ê³„ë¥¼ í¬í•¨í•œ ë…¸ë“œì˜ ìˆ˜ ê³„ì‚°
    no_of_observations = np.arange(2, children.shape[0]+2)

    # ë´ë“œë¡œê·¸ë¨ì„ ê·¸ë¦¬ê¸°ìœ„í•œ ì—°ê²° ë§¤íŠ¸ë¦­ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    linkage_matrix = np.column_stack([children, distance, no_of_observations]).astype(float)

    # ë´ë“œë¡œê·¸ë¨ì„ ê·¸ë¦½ë‹ˆë‹¤.
    dendrogram(linkage_matrix, p = len(data), labels = single_cluster, 
            show_contracted=True, no_labels = True, )
    plt.show()
  ```

    <img width="595" alt="image" src="https://user-images.githubusercontent.com/40768187/177190903-d1ba1911-b53b-4e17-856c-568c7ced844a.png">


#### Complete Linkage

- ì½”ë“œ

    ```python
    complete_clustering = AgglomerativeClustering(n_clusters=3, linkage='complete')

    complete_clustering.fit(data)

    complete_cluster = complete_clustering.labels_
    ```

- ê²°ê³¼
  
  ì‚°ì ë„

  ```python
    plt.scatter(data[:,0], data[:,1], c=complete_cluster)
    plt.title('Sklearn Complete Linkage Hierarchical Clustering')
    plt.show()
  ```

  <img width="384" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2022-07-05 á„‹á…©á„Œá…¥á†« 1 30 16" src="https://user-images.githubusercontent.com/40768187/177192668-3287680a-ce9d-46ca-b0b1-22227a79aeea.png">

  ë´ë“œë¡œê·¸ë¨

  ```python
  from scipy.cluster.hierarchy import dendrogram
    plt.figure(figsize=(10,10))

    # Hierarchical Clusteringì˜ ìì‹ ë…¸ë“œ
    children = complete_clustering.children_

    # ê° ìì‹ ë…¸ë“œê°„ì˜ ê±°ë¦¬ ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆì§€ ì•Šê¸° ë•Œë¬¸ì—, ê· ì¼í•˜ê²Œ ê·¸ë¦¬ë„ë¡ í•©ë‹ˆë‹¤.
    distance = np.arange(children.shape[0])

    # ê° í´ëŸ¬ìŠ¤í„° ë‹¨ê³„ë¥¼ í¬í•¨í•œ ë…¸ë“œì˜ ìˆ˜ ê³„ì‚°
    no_of_observations = np.arange(2, children.shape[0]+2)

    # ë´ë“œë¡œê·¸ë¨ì„ ê·¸ë¦¬ê¸°ìœ„í•œ ì—°ê²° ë§¤íŠ¸ë¦­ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    linkage_matrix = np.column_stack([children, distance, no_of_observations]).astype(float)

    # ë´ë“œë¡œê·¸ë¨ì„ ê·¸ë¦½ë‹ˆë‹¤.
    dendrogram(linkage_matrix, p = len(data), labels = complete_cluster, 
            show_contracted=True, no_labels = True, )
    plt.show()
  ```

    <img width="597" alt="image" src="https://user-images.githubusercontent.com/40768187/177192773-53f95933-5d13-4091-b5a8-15764c930f28.png">

#### Average Linkage

- ì½”ë“œ

    ```python
    average_clustering = AgglomerativeClustering(n_clusters=3, linkage='average')

    average_clustering.fit(data)

    average_cluster = average_clustering.labels_
    ```

- ê²°ê³¼
  
  ì‚°ì ë„

  ```python
    plt.scatter(data[:,0], data[:,1], c=average_cluster)
    plt.title('Sklearn Average Linkage Hierarchical Clustering')
    plt.show()
  ```

  <img width="386" alt="image" src="https://user-images.githubusercontent.com/40768187/177193093-dd7779ad-4737-49f4-bf25-760c7d1cc633.png">

  ë´ë“œë¡œê·¸ë¨

  ```python
    from scipy.cluster.hierarchy import dendrogram
    plt.figure(figsize=(10,10))

    # Hierarchical Clusteringì˜ ìì‹ ë…¸ë“œ
    children = average_clustering.children_

    # ê° ìì‹ ë…¸ë“œê°„ì˜ ê±°ë¦¬ ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆì§€ ì•Šê¸° ë•Œë¬¸ì—, ê· ì¼í•˜ê²Œ ê·¸ë¦¬ë„ë¡ í•©ë‹ˆë‹¤.
    distance = np.arange(children.shape[0])

    # ê° í´ëŸ¬ìŠ¤í„° ë‹¨ê³„ë¥¼ í¬í•¨í•œ ë…¸ë“œì˜ ìˆ˜ ê³„ì‚°
    no_of_observations = np.arange(2, children.shape[0]+2)

    # ë´ë“œë¡œê·¸ë¨ì„ ê·¸ë¦¬ê¸°ìœ„í•œ ì—°ê²° ë§¤íŠ¸ë¦­ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    linkage_matrix = np.column_stack([children, distance, no_of_observations]).astype(float)

    # ë´ë“œë¡œê·¸ë¨ì„ ê·¸ë¦½ë‹ˆë‹¤.
    dendrogram(linkage_matrix, p = len(data), labels = average_cluster, 
            show_contracted=True, no_labels = True, )
    plt.show()
  ```

    <img width="595" alt="image" src="https://user-images.githubusercontent.com/40768187/177193114-c250fd8a-9d18-49ac-9104-6fb48c3eaf23.png">

#### ê°€ì¥ ì¢‹ì€ í´ëŸ¬ìŠ¤í„°ë¥¼ í˜•ì„±í•˜ëŠ” í´ëŸ¬ìŠ¤í„° ìˆ˜

k-means í´ëŸ¬ìŠ¤í„°ë§ê³¼ Average Linkageë¥¼ ì‚¬ìš©í•œ Hierarchical í´ëŸ¬ìŠ¤í„°ë§ì—ì„œ ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ í´ëŸ¬ìŠ¤í„° ìˆ˜ëŠ” ë¬´ì—‡ì¸ì§€ ì•Œì•„ë³¼ ì˜ˆì •

Silhouette ìŠ¤ì½”ì–´ë§ì€ Sklearnì˜ metrics íŒ¨í‚¤ì§€ì— ì¡´ì¬

1. k-means
   
   ```python
   from sklearn.metrics import silhouette_score

   best_n = 1
    best_score = -1

    for n_cluster in range(2, 11):
        kmeans = KMeans(n_clusters=n_cluster)
        kmeans.fit(data)
        cluster = kmeans.predict(data)
        score = silhouette_score(data, cluster)
        
        print('í´ëŸ¬ìŠ¤í„°ì˜ ìˆ˜ : {}, ì‹¤ë£¨ì—£ ì ìˆ˜ : {:.2f}'.format(n_cluster, score))
        if score > best_score :
            best_n = n_cluster
            best_score = score
            
    print('ê°€ì¥ ë†’ì€ ì‹¤ë£¨ì—£ ì ìˆ˜ë¥¼ ê°€ì§„ í´ëŸ¬ìŠ¤í„° ìˆ˜ : {}, ì‹¤ë£¨ì—£ ì ìˆ˜ : {:.2f}'.format(best_n, best_score))
   ```

    í´ëŸ¬ìŠ¤í„°ì˜ ìˆ˜ : 2, ì‹¤ë£¨ì—£ ì ìˆ˜ : 0.49</br>
    í´ëŸ¬ìŠ¤í„°ì˜ ìˆ˜ : 3, ì‹¤ë£¨ì—£ ì ìˆ˜ : 0.57</br>
    í´ëŸ¬ìŠ¤í„°ì˜ ìˆ˜ : 4, ì‹¤ë£¨ì—£ ì ìˆ˜ : 0.49</br>
    í´ëŸ¬ìŠ¤í„°ì˜ ìˆ˜ : 5, ì‹¤ë£¨ì—£ ì ìˆ˜ : 0.45</br>
    í´ëŸ¬ìŠ¤í„°ì˜ ìˆ˜ : 6, ì‹¤ë£¨ì—£ ì ìˆ˜ : 0.42</br>
    í´ëŸ¬ìŠ¤í„°ì˜ ìˆ˜ : 7, ì‹¤ë£¨ì—£ ì ìˆ˜ : 0.38</br>
    í´ëŸ¬ìŠ¤í„°ì˜ ìˆ˜ : 8, ì‹¤ë£¨ì—£ ì ìˆ˜ : 0.38</br>
    í´ëŸ¬ìŠ¤í„°ì˜ ìˆ˜ : 9, ì‹¤ë£¨ì—£ ì ìˆ˜ : 0.38</br>
    í´ëŸ¬ìŠ¤í„°ì˜ ìˆ˜ : 10, ì‹¤ë£¨ì—£ ì ìˆ˜ : 0.38</br>
    ê°€ì¥ ë†’ì€ ì‹¤ë£¨ì—£ ì ìˆ˜ë¥¼ ê°€ì§„ í´ëŸ¬ìŠ¤í„° ìˆ˜ : 3, ì‹¤ë£¨ì—£ ì ìˆ˜ : 0.57

2. Average Linkage Hierarchical Clustering
   
   ```python
    from sklearn.metrics import silhouette_score

    best_n = 1
    best_score = -1

    for n_cluster in range(2, 11):
        average_clustering = AgglomerativeClustering(n_clusters= n_cluster, linkage='average')
        average_clustering.fit(data)
        cluster = average_clustering.labels_
        score = silhouette_score(data, cluster)
        
        print('í´ëŸ¬ìŠ¤í„°ì˜ ìˆ˜ : {}, ì‹¤ë£¨ì—£ ì ìˆ˜ : {:.2f}'.format(n_cluster, score))
        if score > best_score :
            best_n = n_cluster
            best_score = score
            
    print('ê°€ì¥ ë†’ì€ ì‹¤ë£¨ì—£ ì ìˆ˜ë¥¼ ê°€ì§„ í´ëŸ¬ìŠ¤í„° ìˆ˜ : {}, ì‹¤ë£¨ì—£ ì ìˆ˜ : {:.2f}'.format(best_n, best_score))
   ```

í´ëŸ¬ìŠ¤í„°ì˜ ìˆ˜ : 2, ì‹¤ë£¨ì—£ ì ìˆ˜ : 0.49</br>
í´ëŸ¬ìŠ¤í„°ì˜ ìˆ˜ : 3, ì‹¤ë£¨ì—£ ì ìˆ˜ : 0.56</br>
í´ëŸ¬ìŠ¤í„°ì˜ ìˆ˜ : 4, ì‹¤ë£¨ì—£ ì ìˆ˜ : 0.48</br>
í´ëŸ¬ìŠ¤í„°ì˜ ìˆ˜ : 5, ì‹¤ë£¨ì—£ ì ìˆ˜ : 0.42</br>
í´ëŸ¬ìŠ¤í„°ì˜ ìˆ˜ : 6, ì‹¤ë£¨ì—£ ì ìˆ˜ : 0.37</br>
í´ëŸ¬ìŠ¤í„°ì˜ ìˆ˜ : 7, ì‹¤ë£¨ì—£ ì ìˆ˜ : 0.34</br>
í´ëŸ¬ìŠ¤í„°ì˜ ìˆ˜ : 8, ì‹¤ë£¨ì—£ ì ìˆ˜ : 0.34</br>
í´ëŸ¬ìŠ¤í„°ì˜ ìˆ˜ : 9, ì‹¤ë£¨ì—£ ì ìˆ˜ : 0.37</br>
í´ëŸ¬ìŠ¤í„°ì˜ ìˆ˜ : 10, ì‹¤ë£¨ì—£ ì ìˆ˜ : 0.33</br>
ê°€ì¥ ë†’ì€ ì‹¤ë£¨ì—£ ì ìˆ˜ë¥¼ ê°€ì§„ í´ëŸ¬ìŠ¤í„° ìˆ˜ : 3, ì‹¤ë£¨ì—£ ì ìˆ˜ : 0.56

## ğŸ“Œ Regression (íšŒê·€)

- Linear Regression: ì¢…ì† ë³€ìˆ˜ì™€ í•œ ê°œ ì´ìƒì˜ ë…ë¦½ ë³€ìˆ˜ì™€ì˜ ì„ í˜• ìƒê´€ ê´€ê³„ë¥¼ ëª¨ë¸ë§í•˜ëŠ” íšŒê·€ ë¶„ì„ ê¸°ë²•
  - Simple Linear Regression: x (ë…ë¦½ ë³€ìˆ˜) ê°€ 1ê°œì¸ ë‹¨ìˆœ íšŒê·€ ë¶„ì„

    <img width="365" alt="image" src="https://user-images.githubusercontent.com/40768187/177226899-0b7ac3e3-b7de-46b0-a206-115caddbf29c.png">

  - Multiple Linear Regression: x (ë…ë¦½ ë³€ìˆ˜) ê°€ 2ê°œ ì´ìƒì¸ ë‹¤ì¤‘ íšŒê·€ ë¶„ì„

    <img width="366" alt="image" src="https://user-images.githubusercontent.com/40768187/177226953-3a411678-2a96-4f50-b232-58505988f935.png">

- Machine Learning Algorithm Based Regression
  - Decision Tree Regression: ë°ì´í„° ë¶ˆìˆœë„(impurity, Entropy)ë¥¼ ìµœì†Œí™” í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ íŠ¸ë¦¬ë¥¼ ë¶„ê¸°í•˜ì—¬ ëª¨ë¸ ìƒì„±

    <img width="372" alt="image" src="https://user-images.githubusercontent.com/40768187/177227412-7807b3f9-5713-49a9-bb9f-081eebe53e11.png">

  - Support Vector Machine Regressor: ê²°ì • ê²½ê³„ì™€ ê°€ì¥ ê°€ê¹Œìš´ ë°ì´í„° ìƒ˜í”Œì˜ ê±°ë¦¬(Margin) ì„ ìµœëŒ€í™” í•˜ëŠ” ë°©ì‹

    <img width="366" alt="image" src="https://user-images.githubusercontent.com/40768187/177227474-ff2f7dfa-3254-4e10-935b-c70cd66d734e.png">

  - Random Forest Regression
  - MLP Regression: ë”¥ëŸ¬ë‹ì˜ ê¸°ë³¸ ëª¨ë¸ì¸ ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ íšŒê·€ ëª¨ë¸, MLP => ì…ë ¥ì¸µ-ì€ë‹‰ì¸µ-ì¶œë ¥ì¸µ

    <img width="364" alt="image" src="https://user-images.githubusercontent.com/40768187/177227534-9559bc5c-f556-459e-a137-2998b81acff9.png">

  
- í‰ê°€
  - R ì œê³±: í•™ìŠµí•œ íšŒê·€ ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ë°ì´í„°ë¥¼ ì˜ í‘œí˜„í•˜ëŠ”ì§€ì— ëŒ€í•œ ì •ë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í†µê³„ì ì¸ ì²™ë„ (0 < R ì œê³± < 1)
  - Adjusted R ì œê³±: ë³€ìˆ˜ì˜ ìˆ˜ê°€ ì¦ê°€í•˜ëŠ” ê²½ìš°, R ì œê³± ê°’ì€ ëª¨ë¸ ì„±ëŠ¥ì— ê´€ê³„ì—†ì´ ê°’ì´ ìœ ì§€ë˜ê±°ë‚˜ ì¦ê°€í•˜ê²Œ ë¨. ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë‚˜ì˜¨ í‰ê°€ ì²™ë„ (Sklearn ì§€ì›X)


## ğŸ“Œ Classification (ë¶„ë¥˜)

ë¨¸ì‹ ëŸ¬ë‹ê³¼ í†µê³„í•™ì—ì„œì˜ ë¶„ë¥˜ëŠ” ìƒˆë¡œ ê´€ì¸¡ëœ ë°ì´í„°ê°€ ì–´ë–¤ ë²”ì£¼ ì§‘í•©ì— ì†í•˜ëŠ”ì§€ë¥¼ ì‹ë³„í•˜ëŠ” ê²ƒì„ ë§í•¨

í›ˆë ¨ ë°ì´í„°ë¥¼ ì´ìš©í•´ ëª¨ë¸ì„ í•™ìŠµí•˜ë©´, ëª¨ë¸ì€ ê²°ì • ê²½ê³„ (Decision boundary) ë¼ëŠ” ë°ì´í„°ë¥¼ ë¶„ë¥˜í•˜ëŠ” ì„  ë§Œë“¤ì–´ëƒ„

- ì¢…ë¥˜
  - Logistic Regression: ì„ í˜• íšŒê·€ ëª¨ë¸ì—ì„œ ë³€í˜•ëœ ëª¨ë¸

    <img width="536" alt="image" src="https://user-images.githubusercontent.com/40768187/177308865-fdde732e-748c-4bbc-9e06-7895704de2ef.png">

  - SVM: ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•˜ì—¬ ë‘ ì¹´í…Œê³ ë¦¬ ì‚¬ì´ì˜ ê°„ê²©ì„ ìµœëŒ€í™”í•˜ëŠ” ë°ì´í„° í¬ì¸íŠ¸ë¥¼ì°¾ì•„ë‚´ê³ , ê·¸ ì„œí¬íŠ¸ ë²¡í„°ì— ìˆ˜ì§ì¸ ê²½ê³„ë¥¼ í†µí•´ ë°ì´í„° ë¶„ë¥˜í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜

    <img width="543" alt="image" src="https://user-images.githubusercontent.com/40768187/177308595-ad7ff8bf-cd9f-4de6-8d71-635814c565be.png">

    <img width="361" alt="image" src="https://user-images.githubusercontent.com/40768187/177308762-17e21677-9579-4d7d-ad99-feea3a6358c3.png">

  - Decision Tree: ì…ë ¥ ë³€ìˆ˜ë¥¼ íŠ¹ì •í•œ ê¸°ì¤€ìœ¼ë¡œ ì˜ë¼(ë¶„ê¸°) íŠ¸ë¦¬ í˜•íƒœì˜ êµ¬ì¡°ë¡œ ë¶„ë¥˜í•˜ëŠ” ëª¨ë¸, ë°ì´í„°ì˜ ë¶ˆìˆœë„ë¥¼ ìµœì†Œí™”í•  ìˆ˜ ìˆëŠ” ë°©í–¥ìœ¼ë¡œ íŠ¸ë¦¬ ë¶„ê¸°
    - ë¶ˆìˆœë„? ì •ë³´ ì´ë¡ ì—ì„œ ë§í•˜ëŠ” ì–»ì„ ìˆ˜ ìˆëŠ” ì •ë³´ëŸ‰ì´ ë§ì€ ì •ë„
  - Random Forest: ì‘ì€ íŠ¸ë¦¬ë“¤ì„ ì—¬ëŸ¬ê°œë§Œë“¤ì–´ í•©ì¹˜ëŠ” ëª¨ë¸

- í‰ê°€
  - Accuracy: ëª¨ë“  ë°ì´í„°ì— ëŒ€í•´ í´ë˜ìŠ¤ ë¼ë²¨ì„ ì–¼ë§ˆë‚˜ ì˜ ë§ì·„ëŠ”ì§€ ê³„ì‚°
  - Confution Matrix: ëª¨ë¸ì˜ ëª©ì ì— ë§ê²Œ ë¶„ë¥˜ ëª¨ë¸ í‰ê°€
    - Precision, Sensitivity, Specificity, False Alarm
  - ROC Curve, AUC: ë¯¼ê°ë„ì™€ íŠ¹ì´ë„ê°€ ì„œë¡œ ì–´ë–¤ ê´€ê³„ë¥¼ ê°€ì§€ë©° ë³€í•˜ëŠ”ì§€ 2ì°¨ì› í‰ë©´ ìƒì— í‘œí˜„í•œ ê²ƒ
    - Curve: ê·¸ë ¤ì§€ëŠ” ê³¡ì„ 
    - AUC: ROC Curveì˜ ë©´, 1ì— ê°€ê¹Œìš¸ ìˆ˜ë¡ ì¢‹ì€ ëª¨ë¸

</br>

### ì¶”ê°€ ë©”ëª¨

<img width="783" alt="image" src="https://user-images.githubusercontent.com/40768187/177309368-7bebfb8c-f017-4bdb-905f-e9bcb2191547.png">

</br>

- ì§€ë„ í•™ìŠµ: ë¬¸ì œ, ë‹µ ë‘˜ ë‹¤ ì£¼ê³ , ë¹„ìŠ·í•œ ê±° ë‚˜ì˜¤ë©´ ë§ì¶¤ ex) Classification
- ë¹„ì§€ë„ í•™ìŠµ: íŠ¹ì •í•œ í•™ìŠµ ì—†ì´ ê°ì´ ìƒê¸°ëŠ” ê²ƒ. ë°ì´í„°ë¥¼ ë³´ê³  ì•Œì•„ì„œ ë¶„ë¥˜ ex) Clustering

</br>

## ğŸ“Œ Azure ML íŒŒì´í”„ë¼ì¸

### Azure ML Notebooks ì‚¬ìš©í•˜ê¸°
</br>

https://github.com/yejin25/SKT-FLYAI/blob/master/1%EC%A3%BC%EC%B0%A8/0630-Flask%2BAzureML.md

ìœ„ ë§í¬ì—ì„œ Azure ML ì„ ë§Œë“¤ê³ , Designer, Automated ML ì„ ì‚¬ìš©í•œ ì ì´ ìˆìŒ

ì´ë²ˆì—” Notebooks ì—ì„œ Python ì½”ë“œë¥¼ ì‘ì„±í•  ì˜ˆì •

</br>

1. Notebooks ì‹œì‘í•˜ê¸°

    <img width="1605" alt="image" src="https://user-images.githubusercontent.com/40768187/177312854-2107273b-7f95-494f-a141-a466f1b436cc.png">


2.  ```+ Create``` > ```Create new file``` ì„ íƒ í›„ íŒŒì¼ ìƒì„±

    <img width="1614" alt="image" src="https://user-images.githubusercontent.com/40768187/177313067-43b1eb4d-b9c1-4308-bea0-96e6648f9c30.png">

3. Compute instance ì„ íƒ
    
    <img width="1168" alt="image" src="https://user-images.githubusercontent.com/40768187/177315355-0fb3a65f-27b3-4ba0-a5c0-0aa46894e589.png">

    ìƒì„±ëœ ì¸ìŠ¤í„´ìŠ¤ê°€ ì—†ë‹¤ë©´ ìƒì„±í•˜ëŸ¬ ã„±ã„± ì´ë¯¸ í•´ë´¤ìœ¼ë‹ˆê¹ ì˜í• ë“¯

4. Python ì½”ë“œ ì‘ì„±
    
    <img width="1168" alt="image" src="https://user-images.githubusercontent.com/40768187/177315504-892d1aec-cfc0-478f-b3c0-9ffce51de50c.png">

    ì½”ë“œëŠ” ì‹¤í–‰ ë²„íŠ¼ ë§ê³ ë„ ```shift``` + ```Enter``` ë¥¼ ëˆŒëŸ¬ì£¼ë©´ ì‹¤í–‰ë¨

5. ìƒ˜í”Œ ì½”ë“œ ì„¤ëª…

    ì‘ì—…ê³µê°„ ìƒì„±

   ```python
    from azureml.core import Workspace
    ws = Workspace.from_config()
    print('Workspace name:' + ws.name)
   ```

   ì‹¤í—˜ê³µê°„ ìƒì„±

   ```python
    from azureml.core import Experiment
    experiment = Experiment(workspace=ws, name='diabetes-experiment')
   ```

   ë°ì´í„° ì¤€ë¹„

   ```python
    from azureml.opendatasets import Diabetes
    from sklearn.model_selection import train_test_split

    x_df = Diabetes.get_tabular_dataset().to_pandas_dataframe().dropna()
    y_df = x_df.pop("Y")

    X_train, X_test, y_train, y_test = train_test_split(x_df, y_df, test_size=0.2, random_state=66)

    print(X_train)
   ```

   ëª¨ë¸ í›ˆë ¨í•˜ë©´ì„œ ë¡œê·¸ ë‚¨ê¸°ê³  ëª¨ë¸ íŒŒì¼ ì—…ë¡œë“œ

   ```python
    from sklearn.linear_model import Ridge
    from sklearn.metrics import mean_squared_error
    from sklearn.externals import joblib
    import math

    alphas = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]

    for alpha in alphas:
        run = experiment.start_logging()
        run.log('alpha_value', alpha)

        model = Ridge(alpha=alpha)
        model.fit(X=X_train,y=y_train)

        y_pred=model.predict(X=X_test)
        
        rmse = mean_squared_error(y_true=y_test, y_pred=y_pred)
        run.log('rmse', rmse)

        model_name = 'model_alpha_' + str(alpha) + '.pkl'
        filename = 'outputs/' + model_name

        joblib.dump(value=model, filename=filename)
        run.upload_file(name=model_name, path_or_stream=filename)
        run.complete()

        print(f'{alpha} exp completed')     # f: alpha ê°’ì„ ì¹˜í™˜í•´ì£¼ëŠ” ê±°?
   ```

    ì•„ë˜ì™€ ê°™ì´ ë¡œê·¸ë¥¼ ë‚¨ê¸°ë©´ì„œ í›ˆë ¨ì‹œí‚¤ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŒ

   <img width="1165" alt="image" src="https://user-images.githubusercontent.com/40768187/177316965-851e355f-de7f-4a60-af40-95aad575501e.png">

   </br>

    íŒŒì¼ ëª©ë¡ì„ ìƒˆë¡œê³ ì¹¨í•´ì£¼ë©´ outputs í´ë”ë¥¼ ìƒê²¼ê³ , ëª¨ë¸ íŒŒì¼ë“¤ì´ ì—…ë¡œë“œ ë˜ì–´ ìˆëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŒ

   <img width="638" alt="image" src="https://user-images.githubusercontent.com/40768187/177317462-52407ffe-07fe-40a2-b2be-dc31df6d21a3.png">

   </br>

    Studio ì—ì„œ ì‹¤í—˜ ê²°ê³¼ í™•ì¸ ë° ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

    ```python
    experiment
    ```
    
    <img width="1135" alt="image" src="https://user-images.githubusercontent.com/40768187/177317797-522e899f-3cf0-47fc-ab5a-fc6fc4fb9bde.png">

    Report Page í™•ì¸í•´ë³´ê¸°

    <img width="1599" alt="image" src="https://user-images.githubusercontent.com/40768187/177317876-c6989b8d-4664-4126-a48d-973850e4d1c4.png">

    </br>

    Best model íƒìƒ‰ í›„ ë‹¤ìš´ë¡œë“œ

   ```python
    minimum_rmse_runid = None
    minimum_rmse = None

    for run in experiment.get_runs():
        run_metrics = run.get_metrics()
        run_details = run.get_details()
        # each logged metric becomes a key in this returned dict
        run_rmse = run_metrics["rmse"]
        run_id = run_details["runId"]
        
        if minimum_rmse is None:
            minimum_rmse = run_rmse
            minimum_rmse_runid = run_id
        else:
            if run_rmse < minimum_rmse:
                minimum_rmse = run_rmse
                minimum_rmse_runid = run_id

    print("Best run_id: " + minimum_rmse_runid)
    print("Best run_id rmse: " + str(minimum_rmse))
   ```

   ```python
   from azureml.core import Run
    best_run = Run(experiment=experiment, run_id=minimum_rmse_runid)
    print(best_run.get_file_names())
   ```

   ```python
   best_run.download_file(name=str(best_run.get_file_names()[0]))
   ```


    <img width="1031" alt="image" src="https://user-images.githubusercontent.com/40768187/177318578-362dbb48-d0f0-402c-8373-bb4dbf502bdd.png">

    ```model_alpha_0.1.pkl``` ì´ë¼ëŠ” ë² ìŠ¤íŠ¸ ëª¨ë¸ íŒŒì¼ì´ ë‹¤ìš´ë¡œë“œ ëœ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŒ

    </br>

    DataStore ì— Input/Output ë°ì´í„°ì…‹ ë“±ë¡

   ```python
    import numpy as np
    from azureml.core import Dataset

    np.savetxt('features.csv', X_train, delimiter=',')
    np.savetxt('labels.csv', y_train, delimiter=',')

    datastore = ws.get_default_datastore()
    datastore.upload_files(files=['./features.csv', './labels.csv'],
                        target_path='diabetes-experiment/',
                        overwrite=True)

    input_dataset = Dataset.Tabular.from_delimited_files(path=[(datastore, 'diabetes-experiment/features.csv')])
    output_dataset = Dataset.Tabular.from_delimited_files(path=[(datastore, 'diabetes-experiment/labels.csv')])
   ```
    datastore.upload_filesí•¨ìˆ˜ëŠ” deprecated ë˜ì—ˆë‹¤ê³  í•¨</br>
    FileDatasetFactory.upload_directory ëŒ€ì‹  ì“°ë¼ê³  í–ˆì§€ë§Œ, íŒŒì´ì¬ì„ ëª°ë¼ì„œ ì¼ë‹¨ íŒ¨ìŠ¤

    <img width="1397" alt="image" src="https://user-images.githubusercontent.com/40768187/177324035-720b87c0-c6de-4a06-9234-ff8eb356b79a.png">
    íŒŒì¼ ëª©ë¡ì— csv íŒŒì¼ ì¶”ê°€ëœ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŒ

    </br>

    Best model ë“±ë¡

   ```python
    import sklearn

    from azureml.core import Model
    from azureml.core.resource_configuration import ResourceConfiguration


    model = Model.register(workspace=ws,
                        model_name='diabetes-experiment-model',
                        model_path=f"./{str(best_run.get_file_names()[0])}", 
                        model_framework=Model.Framework.SCIKITLEARN,  
                        model_framework_version=sklearn.__version__,  
                        sample_input_dataset=input_dataset,
                        sample_output_dataset=output_dataset,
                        resource_configuration=ResourceConfiguration(cpu=1, memory_in_gb=0.5),
                        description='Ridge regression model to predict diabetes progression.',
                        tags={'area': 'diabetes', 'type': 'regression'})

    print('Name:', model.name)
    print('Version:', model.version)
   ```

   <img width="1613" alt="image" src="https://user-images.githubusercontent.com/40768187/177325262-51a79420-9005-43d6-a126-9540013f0002.png">

    ëª¨ë¸ ë°°í¬

   ```python
    service_name = 'diabetes-service'

    service = Model.deploy(ws, service_name, [model], overwrite=True)
    service.wait_for_deployment(show_output=True)
   ```

   ë°°í¬ ì„œë¹„ìŠ¤ í…ŒìŠ¤íŠ¸

   - ë…¸íŠ¸ë¶

        ```python
        import json

        input_payload = json.dumps({
            'data': X_train[0:2].values.tolist(),
            'method': 'predict'
        })

        output = service.run(input_payload)

        print(output)
        ```

        <img width="578" alt="image" src="https://user-images.githubusercontent.com/40768187/177325087-95e6d205-2330-493d-8c11-05c60993c6b5.png">
        
    - [Models]-[Endpoints]-[ì„œë¹„ìŠ¤ëª…]-[Test]

        <img width="742" alt="image" src="https://user-images.githubusercontent.com/40768187/177325569-16183b44-452b-466b-8dfa-939b02e15b3b.png">

    ì„œë¹„ìŠ¤ ì‚­ì œ

    ```python
    service.delete()
    ```