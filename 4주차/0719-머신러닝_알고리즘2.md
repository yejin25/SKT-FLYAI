### 의사 결정 나무 (Decision Tree)

깨끗하게 두갈래로 나눠지도록 하는 조건이 중요
깊이를 정해줌, 규제를 해주지 않으면 계속해서 만들어냄. -> overfitting 되기 쉬움
깊이 정해주거나 말단 노드가 10개가 될 때까지만 동작하도록 규제가능 (개발자가 넣어줄 파라미터)

### 불순도 

- gini 불순도 (낮은게 불순물 적은거)
- 엔트로피

부모노드와 자식 노드의 gini 계수의 차이를 통해 정보이득이 큰거부터 질문을 만들어 나감

### 특징인가 이건 내가 작성하고도 모르겠네
scaling 필요 없음. 좌표로 하는게 아니라 조건으로만 하는 알고리즘이라서

이론적으론 encoding을 안해도 된다고 함

다중분류 성능 좋음

약한 분류기(트리)를 여러개 써서 앙상블을 만들어서 다수결 원칙으로 모델 뽑음 <- ?

### 정보 획득량
- 하나의 질문에 의해서 감소되는 불순도의 정도

### 규제
- 과적합 방지를 위해 결정트리의 규제 추가 가능

Max_depth  트리 깊이 제한
Min_sample_split  분기를 하기 ㅜ이한 최소한의 샘플수
Min_sample_leaf 리프 노드 가지고 ㅣㅇㅆ어야할 최소한 샘플수
Max_leaf_nodes: 리프노드 최대수
(sample = 데이터 개수)

+)
### 랜덤포레스트

트리 200개 만드는데, 각 트리마다 10개 feature 중 8개 써서, 다 다르게 할당
가장 많은 의견이 나오는 쪽이 결과. (앙상블)


### 앙상블
강력한 하나의 모델을 사용하는 대신 약한 모델 여러 개를 조합하여 더 정확한 예측에 도움주는 방식

1. 보팅 (Voting) - 각 알고리즘이 투표권 1개씩 가지고, 가장 잘 나온 거에 투표
2. 배깅 (Bagging) - 랜덤포레스트의 경우 트리를 다양한 종류로 여러개 만듬, 데이터도 다른 걸 집어넣음

트리를 기반으로 배깅하는 것 = 랜덤 포레스트
배깅은 동시에 흘려보내서 voting.

3. 부스팅 (Boosting) - 여러개를 차례차례 흘려보내기. 이전트리에서 안되던걸(반영해서) 다음트리에서 잘되도록 바꾸면서 진행


부스팅이 가장 잘 나오지만, 시간이 오래 걸림


### 보팅 분류

같은 학습 데이터를 사용해서 여러 종류의 분류기 학습
각각의 분류기에서 나오는 결과에서 다수의 결과 선택
(하드보팅) 1, 0 으로 뽑아서 voting
<그림 찾기>

소프트 보팅 분류
각 분류기에서 클래스별 확률 출력
클래스별 예측확률의 평균이 큰 클래스 선택
<그림 찾기>

### 배깅
학습데이터에서 여러 서브셋을 생성하고, 같은 종류의 분류기 사용

부트스트랩: 데이터로부터 복원(중복허용) 추출.
 classifier -> randomforest


#### 랜덤 포레스트 

배깅과 랜덤트리 (랜덤피쳐스플릿)방법을 적용한 결정 트리의 앙상블 (sklearn- default 트리 100개)

### 부스팅

속도 향상 (속도>성능)
앞의 분류기의 에러를 보완하는 약한 분류기를 차례로(sequential) 연결하는 앙상블 기법

아다부트스(Adaptive boost), 그래디언트 부스팅(gradient boosting)


#### 아다부스트

앞의 분류기에서 예측하지 못했던 데이터의 가중치 높임
가중치 높인 데이터 사용해서 새로운 분류기 만듬
즉, 이전 분류기가 잘못 분류한 데이터에 더 집중하여 학습되도록 함

#### 그래디언트 부스팅
아다부스트와 비슷, 가중치 주는 방식이 경사하강법



### 나이브 베이즈

#### 확률 

#### 조건부 확률
한 사건이 일어났다는 전제 하ㅔㅇ서 다른 사건이 일어날 확률
사건 b가 발생했다는 전제 하에서 사건 A가 일어날 확률


-비가 온 날에 교통사고 날 확률
- URL이 3개 이상 들어있는 메일이 스팸일 확률


--> 각 사건을 독립 사건으로 가절하고, 베이즈 정리를 적용
