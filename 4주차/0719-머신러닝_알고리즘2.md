# 4주차 2022 07 19
</br>

## 📌 Machine Learning 2


### 의사 결정 나무 (Decision Tree)

![image](https://user-images.githubusercontent.com/40768187/180230872-b1a996b5-7807-4cb0-9b40-55c0c4653e95.png)

- 깨끗하게 두갈래로 나눠지도록 하는 조건이 중요
- 깊이를 정해줌, 규제를 해주지 않으면 계속해서 만들어냄. -> overfitting 되기 쉬움
- 깊이 정해주거나 leaf 노드가 10개가 될 때까지만 동작하도록 규제가능 (개발자가 넣어줄 파라미터)

</br>

### 특징

- scaling 필요 없음 -> 좌표로 하는게 아니라 조건으로만 하는 알고리즘이라서
- 이론적으론 encoding을 안해도 됨
- 다중분류 성능 좋음
- 약한 분류기(트리)를 여러개 써서 앙상블을 만들어서 다수결 원칙으로 모델 뽑음

    <img width="384" alt="image" src="https://user-images.githubusercontent.com/40768187/180234692-4ea80cad-3444-4977-8548-39cd9d19ce5a.png">

    classifer = 강분류기 (다중 분류 어려움), 의사결정나무 = 약분류기

</br>

### 규제

- 과적합 방지를 위해 결정트리의 규제 추가 가능

1. max_depth :  트리 깊이 제한
2. min_samples_split : 분기를 하기 위한 최소한의 샘플수
3. min_samples_leaf : 리프 노드 가지고 있어야할 최소한 샘플수
4. max_leaf_nodes : 리프 노드 최대수

(sample = 데이터 개수)

</br>

### 불순도 (Impurity)

![image](https://user-images.githubusercontent.com/40768187/180232250-6e9573ce-e44f-4f16-986a-b96925765799.png)

- gini 불순도 (낮은게 불순물 적은거)

    <img width="215" alt="image" src="https://user-images.githubusercontent.com/40768187/180233224-42da00f4-53b4-45b0-9728-ae5fb0d33c4b.png">

    지니 지수의 최댓값 = 0.5

- 엔트로피

부모 노드와 자식 노드의 gini 계수의 차이 (= 정보 획득량)이 큰거부터 질문을 만들어 나감

</br>

### 정보 획득량

- 하나의 질문에 의해서 감소되는 불순도의 정도

</br>

### 앙상블 (esemble)

강력한 하나의 모델을 사용하는 대신 약한 모델 여러 개를 조합하여 더 정확한 예측에 도움주는 방식</br>
(각 알고리즘이 투표권 1개씩 가지고, 가장 잘 나온 거에 투표)

1. 보팅 (Voting) - 서로 다른 알고리즘을 가진 분류기가 같은 데이터셋을 기반으로 학습되고 결합하는 것

2. 배깅 (Bagging) - 같은 알고리즘 유형의 모델(랜덤 포레스트)들이지만, 데이터 샘플링을 다르게 하여 학습 데이터 셋이 각각 다름 (데이터 셋 중첩 허용)

3. 부스팅 (Boosting)
   - 여러 개를 차례차례 흘려보내기. 이전 트리에서 안되던걸(반영해서) 다음트리에서 잘되도록 바꾸면서 진행
   - 순차학습을 통해 잘못 분류된 샘플에 가중치를 부여해 오차를 줄여나감

</br>

+)

트리를 기반으로 배깅하는 것 = 랜덤 포레스트</br>
부스팅과 달리 배깅은 동시에 흘려보내서 voting</br>
부스팅이 가장 잘 나오지만, 시간이 오래 걸림

</br>

### 보팅 분류

- 같은 학습 데이터를 사용해서 여러 종류의 분류기 학습
- 각각의 분류기에서 나오는 결과에서 다수가 선택한 결과 선택

<img width="434" alt="image" src="https://user-images.githubusercontent.com/40768187/180249077-555bb14d-4500-42bc-aa8f-d14faddcc9d0.png">

</br>

#### - 하드보팅

1, 0 으로 뽑아서 voting

<img width="591" alt="image" src="https://user-images.githubusercontent.com/40768187/180256785-be3c94f0-7c39-4bfc-b2a0-274e600d2928.png">

</br>

#### - 소프트 보팅 분류

- 각 분류기에서 클래스별 확률 출력
- 클래스별 예측확률의 평균이 큰 클래스 선택

<img width="609" alt="image" src="https://user-images.githubusercontent.com/40768187/180257014-8dced758-50ba-4c4f-861f-cca15f9ec71a.png">

</br>

### 배깅

학습데이터에서 여러 서브셋을 생성하고, 같은 종류의 분류기 사용

<img width="444" alt="image" src="https://user-images.githubusercontent.com/40768187/180249332-74faf6ea-20e4-4e0c-9ddd-373c26799402.png">

</br>

**부트스트랩**: 데이터로부터 복원(중복허용) 추출
</br>

<img width="688" alt="image" src="https://user-images.githubusercontent.com/40768187/180259153-abff0470-4c42-4ce0-80a6-7f781136ebd1.png">

</br>

#### 랜덤 포레스트 

배깅과 랜덤트리 (랜덤피쳐스플릿)방법을 적용한 결정 트리의 앙상블 (sklearn - default 트리 100개)

![image](https://user-images.githubusercontent.com/40768187/180240310-48593d31-b164-4a6b-b526-090f52d761ba.png)

- 트리 100개 만드는데, 각 트리마다 10개 feature 중 8개 써서, feature를 다 다르게 할당
- 가장 많은 의견이 나오는 쪽이 결과 -> 앙상블

</br>

### 부스팅

- 앞의 분류기의 에러를 보완하는 약한 분류기를 차례로(sequential) 연결하는 앙상블 기법
- 분류가 어려운 사례를 강조하기 위해 잘못 분류한 데이터에 가중치를 주어 더 잘 학습하도록 함
- 잘못 분류된 데이터일수록 샘플링될 확률이 늘어남
- XGBoosting 이후 알고리즘에서 속도 문제 해결

 ![image](https://user-images.githubusercontent.com/40768187/180255934-72eb387d-c325-4dad-a50c-7d7ce8243c09.png)

 </br>
 
### 부스팅 종류

- 아다부트스(Adaptive boost)
- 그래디언트 부스팅(gradient boosting)


#### 아다부스트

![2222](https://user-images.githubusercontent.com/40768187/180262368-8a3c7b57-20fc-4a6a-9f23-c0b848b81036.png)

- 앞의 분류기에서 예측하지 못했던 데이터의 가중치 높임
- 가중치 높인 데이터 사용해서 새로운 분류기 만듬
- 이전 분류기가 잘못 분류한 데이터에 더 집중하여 학습되도록 함

</br>

#### 그래디언트 부스팅

아다부스트와 비슷, 가중치 주는 방식이 **경사하강법**

</br>

### 부스팅 라이브러리

- XGBoost
- LightGBM
- CatBoost

</br>

#### 스태킹

라이브러리(위에 3개)로 분류 후 학습시켜서 결과를 다시 모아서 한번 더 머신러닝 알고리즘을 또 돌림</br>
~~잘안씀 결과 잘 안좋음~~

</br>

### 나이브 베이즈

<img width="380" alt="image" src="https://user-images.githubusercontent.com/40768187/180264889-e7c47ec9-d90c-47d9-b87e-30bb2f2caca0.png">

</br >

#### 나이브 베이즈 수식

- 각 사건을 독립 사건이라 생각하고 베이즈 정리 적용
- 사후확률에 영향을 미치는 모든 원인, 특징들의 가중치가 동일하다고 가정 (그래서 naive)

<img width="383" alt="image" src="https://user-images.githubusercontent.com/40768187/180263223-03ee48e0-f58b-46da-9d90-f9de3687503b.png">

</br>

#### 확률 

<img width="348" alt="image" src="https://user-images.githubusercontent.com/40768187/180262614-72567a90-d7a9-473d-97e0-2292f9d910ac.png">

</br>

#### 조건부 확률

<img width="274" alt="image" src="https://user-images.githubusercontent.com/40768187/180262700-b5ecc4da-a971-4fe8-8837-1a2f06b50136.png">

사건 b가 발생했다는 전제 하에서 사건 A가 일어날 확률

- 비가 온 날에 교통사고 날 확률
- URL이 3개 이상 들어있는 메일이 스팸일 확률

</br>

#### 베이즈 정리

- 사전 정보(데이터)를 통해 사후확률(특징이 주어졌을 때 판단)을 예측하는 과정에서 사용되는 공식
- 관찰을 통해 새로운 정보를 획득하면서 사후 확률을 업데이트

<img width="405" alt="image" src="https://user-images.githubusercontent.com/40768187/180263035-aea5e0d5-a089-4894-aba9-28ae3f23357e.png">

</br>

참고 사이트 - [나이브 베이즈 설명 잘해놓은 블로그](https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-1%EB%82%98%EC%9D%B4%EB%B8%8C-%EB%B2%A0%EC%9D%B4%EC%A6%88-%EB%B6%84%EB%A5%98-Naive-Bayes-Classification)
